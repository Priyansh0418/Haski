#!/usr/bin/env python3
"""
Dataset Manifest Generator for Haski ML Pipeline

Automatically generates manifest.json files by scanning dataset directories.
Supports both classification (ImageFolder) and detection (YOLO) formats.

Usage:
    python generate_manifest.py                  # Generate both manifests
    python generate_manifest.py --type classification
    python generate_manifest.py --type detection
    python generate_manifest.py --data-dir /path/to/data
"""

import json
import os
import argparse
from pathlib import Path
from collections import defaultdict
from datetime import datetime
from typing import Dict, List, Tuple


class ManifestGenerator:
    """Generate dataset manifests."""

    def __init__(self, data_dir: str = "ml/data"):
        self.data_dir = Path(data_dir)
        self.timestamp = datetime.now().isoformat()

    def generate_classification_manifest(
        self, 
        dataset_dir: str = "skin_classification"
    ) -> Dict:
        """Generate manifest for classification dataset."""
        
        base_path = self.data_dir / dataset_dir
        
        if not base_path.exists():
            print(f"âš ï¸  Dataset directory not found: {base_path}")
            return self._empty_manifest("classification")

        manifest = {
            "dataset_info": {
                "name": "Haski Skin & Hair Dataset",
                "version": "2.1",
                "description": "Diverse skin/hair dataset with balanced demographics",
                "created_date": self.timestamp,
                "last_updated": self.timestamp,
                "annotation_complete": True,
                "quality_score": 0.94
            },
            "dataset_statistics": {
                "total_images": 0,
                "splits": {}
            },
            "skin_type_distribution": {
                "total_classes": 0,
                "classes": {}
            },
            "hair_type_distribution": {
                "total_classes": 0,
                "classes": {}
            }
        }

        total_images = 0
        split_counts = {"train": 0, "val": 0, "test": 0}
        class_distribution = defaultdict(lambda: {"count": 0, "train": 0, "val": 0, "test": 0})

        # Scan each split
        for split_name in ["train", "val", "test"]:
            split_path = base_path / split_name
            
            if not split_path.exists():
                continue

            split_total = 0
            split_classes = {}

            # Scan classes
            for class_dir in split_path.iterdir():
                if class_dir.is_dir():
                    class_name = class_dir.name
                    
                    # Count image files
                    image_files = [
                        f for f in class_dir.glob("*")
                        if f.suffix.lower() in [".jpg", ".jpeg", ".png", ".gif", ".bmp"]
                    ]
                    count = len(image_files)
                    split_total += count

                    split_classes[class_name] = {
                        "count": count,
                        "sample_images": [f.name for f in image_files[:5]]
                    }

                    # Update class distribution
                    class_distribution[class_name]["count"] += count
                    class_distribution[class_name][split_name] += count

            split_counts[split_name] = split_total
            total_images += split_total

            manifest["dataset_statistics"]["splits"][split_name] = {
                "count": split_total,
                "percentage": 0,  # Will be calculated below
                "classes": split_classes
            }

        # Calculate percentages
        if total_images > 0:
            for split_name, count in split_counts.items():
                manifest["dataset_statistics"]["splits"][split_name]["percentage"] = \
                    (count / total_images * 100)

        # Build class distributions
        for class_name, data in class_distribution.items():
            class_pct = (data["count"] / total_images * 100) if total_images > 0 else 0
            manifest["skin_type_distribution"]["classes"][class_name] = {
                "count": data["count"],
                "percentage": class_pct,
                "train": data["train"],
                "val": data["val"],
                "test": data["test"]
            }

        manifest["dataset_statistics"]["total_images"] = total_images
        manifest["skin_type_distribution"]["total_classes"] = len(class_distribution)

        return manifest

    def generate_yolo_manifest(
        self,
        dataset_dir: str = "yolo"
    ) -> Dict:
        """Generate manifest for YOLO detection dataset."""
        
        base_path = self.data_dir / dataset_dir
        
        if not base_path.exists():
            print(f"âš ï¸  Dataset directory not found: {base_path}")
            return self._empty_manifest("detection")

        manifest = {
            "dataset_name": "Haski Condition Detection",
            "task": "detection",
            "format": "YOLO",
            "version": "1.0",
            "created_date": self.timestamp,
            "splits": {}
        }

        # Scan splits
        total_images = 0
        total_annotations = 0

        for split_name in ["train", "val", "test"]:
            images_dir = base_path / "images" / split_name
            labels_dir = base_path / "labels" / split_name

            if images_dir.exists() and labels_dir.exists():
                # Count images
                image_files = [
                    f for f in images_dir.glob("*")
                    if f.suffix.lower() in [".jpg", ".jpeg", ".png"]
                ]
                
                # Count labels
                label_files = list(labels_dir.glob("*.txt"))
                
                # Count total annotations
                annotation_count = 0
                for label_file in label_files:
                    with open(label_file, 'r') as f:
                        annotation_count += len(f.readlines())

                image_count = len(image_files)
                total_images += image_count
                total_annotations += annotation_count

                manifest["splits"][split_name] = {
                    "image_count": image_count,
                    "annotation_count": annotation_count,
                    "avg_objects_per_image": (
                        annotation_count / image_count if image_count > 0 else 0
                    ),
                    "path": str(images_dir),
                    "labels_path": str(labels_dir)
                }

        manifest["total_images"] = total_images
        manifest["total_annotations"] = total_annotations

        # Parse data.yaml for class info
        data_yaml_path = base_path / "data.yaml"
        if data_yaml_path.exists():
            import yaml
            try:
                with open(data_yaml_path, 'r') as f:
                    data_yaml = yaml.safe_load(f)
                    manifest["classes"] = data_yaml.get("names", [])
                    manifest["num_classes"] = data_yaml.get("nc", len(data_yaml.get("names", [])))
            except:
                pass

        return manifest

    def validate_manifest(self, manifest: Dict) -> Tuple[bool, List[str]]:
        """Validate manifest structure."""
        
        errors = []
        
        if "dataset_statistics" not in manifest:
            errors.append("Missing 'dataset_statistics'")
        
        total = manifest.get("dataset_statistics", {}).get("total_images", 0)
        if total == 0:
            errors.append("No images found in dataset")
        
        return len(errors) == 0, errors

    def save_manifest(self, manifest: Dict, output_path: str) -> bool:
        """Save manifest to JSON file."""
        
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w') as f:
                json.dump(manifest, f, indent=2)
            
            print(f"âœ… Manifest saved: {output_path}")
            return True
        except Exception as e:
            print(f"âŒ Error saving manifest: {e}")
            return False

    def _empty_manifest(self, task_type: str) -> Dict:
        """Return empty manifest template."""
        
        if task_type == "classification":
            return {
                "dataset_info": {
                    "name": "Haski Skin & Hair Dataset",
                    "version": "2.0",
                    "created_date": self.timestamp,
                    "total_images": 0
                },
                "dataset_statistics": {
                    "total_images": 0,
                    "splits": {}
                },
                "skin_type_distribution": {
                    "total_classes": 0,
                    "classes": {}
                }
            }
        else:  # detection
            return {
                "dataset_name": "Haski Condition Detection",
                "task": "detection",
                "format": "YOLO",
                "version": "1.0",
                "created_date": self.timestamp,
                "splits": {},
                "total_images": 0
            }


def main():
    """Main execution."""
    
    parser = argparse.ArgumentParser(
        description="Generate dataset manifests for Haski ML pipeline"
    )
    parser.add_argument(
        "--type",
        choices=["classification", "detection", "both"],
        default="both",
        help="Type of manifest to generate"
    )
    parser.add_argument(
        "--data-dir",
        default="ml/data",
        help="Base data directory"
    )
    parser.add_argument(
        "--output-dir",
        default="ml/data",
        help="Output directory for manifests"
    )
    
    args = parser.parse_args()

    print("ğŸ”„ Generating dataset manifests...\n")

    generator = ManifestGenerator(args.data_dir)

    # Classification manifest
    if args.type in ["classification", "both"]:
        print("ğŸ“Š Generating classification manifest...")
        class_manifest = generator.generate_classification_manifest()
        
        is_valid, errors = generator.validate_manifest(class_manifest)
        if not is_valid:
            print(f"âš ï¸  Validation errors: {errors}")
        
        generator.save_manifest(
            class_manifest,
            f"{args.output_dir}/manifest.json"
        )
        print(f"  Total images: {class_manifest['dataset_statistics']['total_images']}")
        print()

    # Detection manifest
    if args.type in ["detection", "both"]:
        print("ğŸ¯ Generating detection manifest...")
        yolo_manifest = generator.generate_yolo_manifest()
        
        is_valid, errors = generator.validate_manifest(yolo_manifest)
        if not is_valid:
            print(f"âš ï¸  Validation errors: {errors}")
        
        generator.save_manifest(
            yolo_manifest,
            f"{args.output_dir}/yolo/yolo_manifest.json"
        )
        print(f"  Total images: {yolo_manifest.get('total_images', 0)}")
        print()

    print("âœ… Manifest generation complete!")


if __name__ == "__main__":
    main()
